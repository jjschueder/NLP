{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMU NLP Course—Homework 2\n",
    "## Joseph Schueder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tIn Python, create a method for scoring the vocabulary size of a text, and normalize the score from 0 to 1. It does not matter what method you use for normalization as long as you explain it in a short paragraph. (Various methods will be discussed in the live session.)\n",
    "\n",
    "2.\tAfter consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above.\n",
    "\n",
    "3.\tNow create a “text difficulty score” by combining the lexical diversity score from homework 1, and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. Explain what you see when this score is applied to same graded texts you used in homework 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_size(text):\n",
    "    return len(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_size2(text):\n",
    "    return len(set(word.lower() for word in text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_size2(text):\n",
    "    V = set(word.lower() for word in text)\n",
    "    long_words = [w for w in V if len(w) > 15]\n",
    "    return (len(long_words)), long_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_size(text):\n",
    "    return len(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Method to get the Vocab Size and normalize the score\n",
    "\n",
    "def n_vocab_size(*arg):\n",
    "    vocab_size = np.array([])\n",
    "    vocab_size_norm = np.array([])\n",
    "    \n",
    "    #### Getting the Vocab Size\n",
    "    for text in arg:\n",
    "        vocab_size = np.append(vocab_size,len(set(text)))\n",
    "    \n",
    "    #### Normalizing using the formula \n",
    "    for vsize in vocab_size:\n",
    "        vocab_size_norm = np.append(vocab_size_norm,(vsize - vocab_size.min()) /\n",
    "                                                    (vocab_size.max() - vocab_size.min()))\n",
    "    \n",
    "    #### Normalizing using sklearn preprocessing \n",
    "    vocab_size_norm_sklearn = minmax_scale(vocab_size, feature_range=(0,1), axis=0)\n",
    "    \n",
    "    return(vocab_size,vocab_size_norm,vocab_size_norm_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some corpuses to get average word count. This will be used to later get word counts of those and then to normalize them. This normalization will then be applied to the word count of any inputed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['firefox.txt',\n",
       " 'grail.txt',\n",
       " 'overheard.txt',\n",
       " 'pirates.txt',\n",
       " 'singles.txt',\n",
       " 'wine.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webtext.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firefox.txt Cookie Manager: \"Don't allow sites that set removed cookies to se ...\n",
      "grail.txt SCENE 1: [wind] [clop clop clop] \n",
      "KING ARTHUR: Whoa there!  [clop ...\n",
      "overheard.txt White guy: So, do you have any plans for this evening?\n",
      "Asian girl ...\n",
      "pirates.txt PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terr ...\n",
      "singles.txt 25 SEXY MALE, seeks attrac older single lady, for discreet encoun ...\n",
      "wine.txt Lovely delicate, fragrant Rhone wine. Polished leather and strawb ...\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import webtext\n",
    "textlist = []\n",
    "for fileid in webtext.fileids():\n",
    "    print(fileid, webtext.raw(fileid)[:65], '...')\n",
    "    textlist.append(webtext.raw(fileid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = vocab_size(rawtext)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size2 = vocab_size2(rawtext)\n",
    "size2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In Python, create a method for scoring the vocabulary size of a text, and normalize the score from 0 to 1. It does not matter what method you use for normalization as long as you explain it in a short paragraph. (Various methods will be discussed in the live session.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sizer,vocab_size_normr,vocab_size_norm_sklearnr = n_vocab_size(textlist[0], textlist[1],textlist[2], textlist[3], textlist[4], textlist[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([117.,  76.,  88.,  74.,  77.,  88.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.04651163, 0.3255814 , 0.        , 0.06976744,\n",
       "       0.3255814 ])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_normr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.04651163, 0.3255814 , 0.        , 0.06976744,\n",
       "       0.3255814 ])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_norm_sklearnr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. After consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now create a “text difficulty score” by combining the lexical diversity score from homework 1, and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. Explain what you see when this score is applied to same graded texts you used in homework 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus (corpus, html_stripping=True, contraction_expansion=True,\n",
    "    accented_char_removal=True, text_lower_case=True,\n",
    "    text_lemmatization=True, special_char_removal=True,\n",
    "    stopword_removal=True, remove_digits=True):\n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions\n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text\n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "            doc = re.sub(r'[\\r|\\n|\\r\\n]+', doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits\n",
    "        if special_char_removal:\n",
    "        # insert spaces between special characters to isolate them\n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1\", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)\n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ' , doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "        normalized_corpus.append(doc)\n",
    "    return normalized_corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('ML7331': conda)",
   "language": "python",
   "name": "python37164bitml7331condaf5c5c353a7fa46f5b783ad99991c2b10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
