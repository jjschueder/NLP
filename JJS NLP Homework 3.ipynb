{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "1.\tCompare your given name with your nickname (if you don’t have a nickname, invent one for this assignment) by answering the following questions:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From: https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch07%20-%20Text%20Similarity%20and%20Clustering/Ch07a%20-%20Analyzing%20Term%20Similarity.ipynb\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def levenshtein_edit_distance(u, v):\n",
    "    # convert to lower case\n",
    "    u = u.lower()\n",
    "    v = v.lower()\n",
    "    # base cases\n",
    "    if u == v: return 0\n",
    "    elif len(u) == 0: return len(v)\n",
    "    elif len(v) == 0: return len(u)\n",
    "    # initialize edit distance matrix\n",
    "    edit_matrix = []\n",
    "    # initialize two distance matrices \n",
    "    du = [0] * (len(v) + 1)\n",
    "    dv = [0] * (len(v) + 1)\n",
    "    # du: the previous row of edit distances\n",
    "    for i in range(len(du)):\n",
    "        du[i] = i\n",
    "    # dv : the current row of edit distances    \n",
    "    for i in range(len(u)):\n",
    "        dv[0] = i + 1\n",
    "        # compute cost as per algorithm\n",
    "        for j in range(len(v)):\n",
    "            cost = 0 if u[i] == v[j] else 1\n",
    "            dv[j + 1] = min(dv[j] + 1, du[j + 1] + 1, du[j] + cost)\n",
    "        # assign dv to du for next iteration\n",
    "        for j in range(len(du)):\n",
    "            du[j] = dv[j]\n",
    "        # copy dv to the edit matrix\n",
    "        edit_matrix.append(copy.copy(dv))\n",
    "    # compute the final edit distance and edit matrix    \n",
    "    distance = dv[len(v)]\n",
    "    edit_matrix = np.array(edit_matrix)\n",
    "    edit_matrix = edit_matrix.T\n",
    "    edit_matrix = edit_matrix[1:,]\n",
    "    edit_matrix = pd.DataFrame(data=edit_matrix,\n",
    "                               index=list(v),\n",
    "                               columns=list(u))\n",
    "    return distance, edit_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 a.\tWhat is the edit distance between your nickname and your given name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 3 \n",
      " Matrix: \n",
      "    j  o  s  e  p  h\n",
      "j  0  1  2  3  4  5\n",
      "o  1  0  1  2  3  4\n",
      "e  2  1  1  1  2  3\n"
     ]
    }
   ],
   "source": [
    "distancer, edit_matrixr = levenshtein_edit_distance('Joseph', \"Joe\")\n",
    "print('Distance:', distancer, '\\n', \"Matrix: \\n\", edit_matrixr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. b.\tWhat is the percentage string match between your nickname and your given name?\n",
    "Show your work for both calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(u, v):\n",
    "    distance = 1.0 - (np.dot(u, v) / \n",
    "                        (np.sqrt(sum(np.square(u))) * np.sqrt(sum(np.square(v))))\n",
    "                     )\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tFind a friend (or family member or classmate) who you know has read a certain book. Without your friend knowing, copy the first two sentences of that book. Now rewrite the words from those sentences, excluding stop words. Now tell your friend to guess which book the words are from by reading them just that list of words. Did you friend correctly guess the book on the first try? What did he or she guess? Explain why you think you friend either was or was not able to guess the book from hearing the list of words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from collections import Counter\n",
    "def remove_stopwords(input_text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    words = [word.lower() for word in input_text if word.lower() not in stopwords]\n",
    "    c = Counter(words)\n",
    "    c.most_common(10)\n",
    "    return words, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwordsx(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "#remove_stopwords(\"The, and, if are stopwords, computer is not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Mr. and Mrs. Dursley, of number four, Privet Drive, \\nwere proud to say that they were perfectly normal, \\nthank you very much. They were the last people you’d \\nexpect to be involved in anything strange or \\nmysterious, because they just didn’t hold with such \\nnonsense.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"\"\"\"Mr. and Mrs. Dursley, of number four, Privet Drive, \n",
    "were proud to say that they were perfectly normal, \n",
    "thank you very much. They were the last people you’d \n",
    "expect to be involved in anything strange or \n",
    "mysterious, because they just didn’t hold with such \n",
    "nonsense.\"\"\"\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(input_text):\n",
    "    #rudimentary split into sentences\n",
    "    #input_text = input_text.split('.')\n",
    "    #input_text\n",
    "    #split into a list of words per sentence\n",
    "    tokens = [item.split() for item in input_text]\n",
    "    tokens\n",
    "    #lower case and put into a list of words independent of sentence\n",
    "    words = [word for sentence in tokens for word in sentence]\n",
    "    words = [word.lower() for word in words]\n",
    "    words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenize_words(input_text)\n",
    "#words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "default_st = nltk.sent_tokenize\n",
    "_sentences = default_st(text=input_text)\n",
    "#_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the text: ['\"mr.', 'mrs.', 'dursley,', 'number', 'four,', 'privet', 'drive,', 'proud', 'say', 'perfectly', 'normal,', 'thank', 'much.', 'last', 'people', 'you’d', 'expect', 'involved', 'anything', 'strange', 'mysterious,', 'didn’t', 'hold', 'nonsense.']\n"
     ]
    }
   ],
   "source": [
    "words = tokenize_words(_sentences)\n",
    "wordsr, cr = remove_stopwords(words)\n",
    "print(\"Here is the text:\", wordsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" Mr. Mrs. Dursley , number four , Privet Drive , proud say perfectly normal , thank much. last people ’ expect involved anything strange mysterious , ’ hold nonsense .'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_text = remove_stopwordsx(input_text)\n",
    "filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3.\tRun one of the stemmers available in Python. Run the same two sentences from question 2 above through the stemmer and show the results. How many of the outputted stems are valid morphological roots of the corresponding words? Express this answer as a percentage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-f265680320fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# use spacy.load('en') if you have downloaded the language model en directly after install spacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'My system keeps crashing his crashed yesterday, ours crashes daily'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# use spacy.load('en') if you have downloaded the language model en directly after install spacy\n",
    "nlp = spacy.load('en_core', parse=True, tag=True, entity=True)\n",
    "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "lem_text = lemmatize_text(filtered_text)\n",
    "lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter Stemmer\n",
    "def porter_stemmer(input_list):\n",
    "    from nltk.stem import PorterStemmer\n",
    "    ps = PorterStemmer()\n",
    "    output_list = []\n",
    "    [output_list.append(ps.stem(x)) for x in input_list]\n",
    "    return output_list\n",
    "    #ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancaster Stemmer\n",
    "def lancaster_stemmer(input_list):\n",
    "    from nltk.stem import LancasterStemmer\n",
    "    ls = LancasterStemmer()\n",
    "    output_list = []\n",
    "    [output_list.append(ls.stem(x)) for x in input_list]\n",
    "    return output_list\n",
    "    #ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex based stemmer\n",
    "def regex_stemmer(input_list):\n",
    "    from nltk.stem import RegexpStemmer\n",
    "    rs = RegexpStemmer('ing$|s$|ed$', min=4)\n",
    "    output_list = []\n",
    "    [output_list.append(rs.stem(x)) for x in input_list]\n",
    "    return output_list\n",
    "    #rs.stem('jumping'), rs.stem('jumps'), rs.stem('jumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowball Stemmer\n",
    "def snowball_stemmer(input_list):\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    ss = SnowballStemmer(\"english\")\n",
    "    output_list = []\n",
    "    [output_list.append(ss.stem(x)) for x in input_list]\n",
    "    return output_list\n",
    "    #ss.stem('interstate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"mr.',\n",
       " 'mrs.',\n",
       " 'dursley,',\n",
       " 'number',\n",
       " 'four,',\n",
       " 'privet',\n",
       " 'drive,',\n",
       " 'proud',\n",
       " 'say',\n",
       " 'perfectli',\n",
       " 'normal,',\n",
       " 'thank',\n",
       " 'much.',\n",
       " 'last',\n",
       " 'peopl',\n",
       " 'you’d',\n",
       " 'expect',\n",
       " 'involv',\n",
       " 'anyth',\n",
       " 'strang',\n",
       " 'mysterious,',\n",
       " 'didn’t',\n",
       " 'hold',\n",
       " 'nonsense.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pstem_list = porter_stemmer(wordsr)\n",
    "pstem_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"mr.',\n",
       " 'mrs.',\n",
       " 'dursley,',\n",
       " 'numb',\n",
       " 'four,',\n",
       " 'privet',\n",
       " 'drive,',\n",
       " 'proud',\n",
       " 'say',\n",
       " 'perfect',\n",
       " 'normal,',\n",
       " 'thank',\n",
       " 'much.',\n",
       " 'last',\n",
       " 'peopl',\n",
       " 'you’d',\n",
       " 'expect',\n",
       " 'involv',\n",
       " 'anyth',\n",
       " 'strange',\n",
       " 'mysterious,',\n",
       " 'didn’t',\n",
       " 'hold',\n",
       " 'nonsense.']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancstem_list = lancaster_stemmer(wordsr)\n",
    "lancstem_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"mr.',\n",
       " 'mrs.',\n",
       " 'dursley,',\n",
       " 'number',\n",
       " 'four,',\n",
       " 'privet',\n",
       " 'drive,',\n",
       " 'proud',\n",
       " 'say',\n",
       " 'perfectly',\n",
       " 'normal,',\n",
       " 'thank',\n",
       " 'much.',\n",
       " 'last',\n",
       " 'people',\n",
       " 'you’d',\n",
       " 'expect',\n",
       " 'involv',\n",
       " 'anyth',\n",
       " 'strange',\n",
       " 'mysterious,',\n",
       " 'didn’t',\n",
       " 'hold',\n",
       " 'nonsense.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regstem_list = regex_stemmer(wordsr)\n",
    "regstem_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"mr.',\n",
       " 'mrs.',\n",
       " 'dursley,',\n",
       " 'number',\n",
       " 'four,',\n",
       " 'privet',\n",
       " 'drive,',\n",
       " 'proud',\n",
       " 'say',\n",
       " 'perfect',\n",
       " 'normal,',\n",
       " 'thank',\n",
       " 'much.',\n",
       " 'last',\n",
       " 'peopl',\n",
       " \"you'd\",\n",
       " 'expect',\n",
       " 'involv',\n",
       " 'anyth',\n",
       " 'strang',\n",
       " 'mysterious,',\n",
       " \"didn't\",\n",
       " 'hold',\n",
       " 'nonsense.']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowstem_list = snowball_stemmer(wordsr)\n",
    "snowstem_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('ML7331': conda)",
   "language": "python",
   "name": "python37164bitml7331condaf5c5c353a7fa46f5b783ad99991c2b10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
