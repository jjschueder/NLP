{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.1 64-bit ('ML7331': conda)",
      "language": "python",
      "name": "python37164bitml7331condaf5c5c353a7fa46f5b783ad99991c2b10"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "JJS NLP Homework 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXq-lbU8Yzfw"
      },
      "source": [
        "# Homework 3\n",
        "\n",
        "1.\tCompare your given name with your nickname (if you don’t have a nickname, invent one for this assignment) by answering the following questions:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuymm3fNYzf0"
      },
      "source": [
        "#From: https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch07%20-%20Text%20Similarity%20and%20Clustering/Ch07a%20-%20Analyzing%20Term%20Similarity.ipynb\n",
        "import copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "def levenshtein_edit_distance(u, v):\n",
        "    # convert to lower case\n",
        "    u = u.lower()\n",
        "    v = v.lower()\n",
        "    # base cases\n",
        "    if u == v: return 0\n",
        "    elif len(u) == 0: return len(v)\n",
        "    elif len(v) == 0: return len(u)\n",
        "    # initialize edit distance matrix\n",
        "    edit_matrix = []\n",
        "    # initialize two distance matrices \n",
        "    du = [0] * (len(v) + 1)\n",
        "    dv = [0] * (len(v) + 1)\n",
        "    # du: the previous row of edit distances\n",
        "    for i in range(len(du)):\n",
        "        du[i] = i\n",
        "    # dv : the current row of edit distances    \n",
        "    for i in range(len(u)):\n",
        "        dv[0] = i + 1\n",
        "        # compute cost as per algorithm\n",
        "        for j in range(len(v)):\n",
        "            cost = 0 if u[i] == v[j] else 1\n",
        "            dv[j + 1] = min(dv[j] + 1, du[j + 1] + 1, du[j] + cost)\n",
        "        # assign dv to du for next iteration\n",
        "        for j in range(len(du)):\n",
        "            du[j] = dv[j]\n",
        "        # copy dv to the edit matrix\n",
        "        edit_matrix.append(copy.copy(dv))\n",
        "    # compute the final edit distance and edit matrix    \n",
        "    distance = dv[len(v)]\n",
        "    edit_matrix = np.array(edit_matrix)\n",
        "    edit_matrix = edit_matrix.T\n",
        "    edit_matrix = edit_matrix[1:,]\n",
        "    edit_matrix = pd.DataFrame(data=edit_matrix,\n",
        "                               index=list(v),\n",
        "                               columns=list(u))\n",
        "    return distance, edit_matrix"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayfz7HCOYzf2"
      },
      "source": [
        "1 a.\tWhat is the edit distance between your nickname and your given name?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyxNxo07Yzf2",
        "outputId": "410c3c07-b65f-4b0e-fc67-6d7fcc9cc50f"
      },
      "source": [
        "distancer, edit_matrixr = levenshtein_edit_distance('Joseph', \"Joe\")\n",
        "print('Distance:', distancer, '\\n', \"Matrix: \\n\", edit_matrixr)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distance: 3 \n",
            " Matrix: \n",
            "    j  o  s  e  p  h\n",
            "j  0  1  2  3  4  5\n",
            "o  1  0  1  2  3  4\n",
            "e  2  1  1  1  2  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHKjVQgnYzf3"
      },
      "source": [
        "1. b.\tWhat is the percentage string match between your nickname and your given name?\n",
        "Show your work for both calculations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGEz7TF6Yzf4"
      },
      "source": [
        "def cosine_distance(u, v):\n",
        "    distance = 1.0 - (np.dot(u, v) / \n",
        "                        (np.sqrt(sum(np.square(u))) * np.sqrt(sum(np.square(v))))\n",
        "                     )\n",
        "    return distance"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmELooKeYzf5"
      },
      "source": [
        "2.\tFind a friend (or family member or classmate) who you know has read a certain book. Without your friend knowing, copy the first two sentences of that book. Now rewrite the words from those sentences, excluding stop words. Now tell your friend to guess which book the words are from by reading them just that list of words. Did you friend correctly guess the book on the first try? What did he or she guess? Explain why you think you friend either was or was not able to guess the book from hearing the list of words. \n",
        "\n",
        "The words were from the first paragraph of Harry Potter Sorcerer's Stone. They were able to guess it on the first try. I believe they were able to identify it immediately because one of the words that remained was 'Dursley' this is a proper name and is not used in many places in the english language. It is used in the book quite a bit and she knows the book very well. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGSVCZyNYzf5"
      },
      "source": [
        "import nltk \n",
        "from collections import Counter\n",
        "def remove_stopwords(input_text):\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\n",
        "    words = [word.lower() for word in input_text if word.lower() not in stopwords]\n",
        "    c = Counter(words)\n",
        "    c.most_common(10)\n",
        "    return words, c"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpqVGmsFZrxP",
        "outputId": "e36a6765-1c37-4925-b3e8-3440e2a35c84"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXbPMk3KYzf6"
      },
      "source": [
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "def remove_stopwordsx(text, is_lower_case=False, stopwords=stopword_list):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "\n",
        "#remove_stopwords(\"The, and, if are stopwords, computer is not\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "vBgLfiMmYzf6",
        "outputId": "2e3200c1-990f-43f5-8bec-83b642134274"
      },
      "source": [
        "input_text = \"\"\"\"Mr. and Mrs. Dursley, of number four, Privet Drive, \n",
        "were proud to say that they were perfectly normal, \n",
        "thank you very much. They were the last people you’d \n",
        "expect to be involved in anything strange or \n",
        "mysterious, because they just didn’t hold with such \n",
        "nonsense.\"\"\"\n",
        "input_text"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"Mr. and Mrs. Dursley, of number four, Privet Drive, \\nwere proud to say that they were perfectly normal, \\nthank you very much. They were the last people you’d \\nexpect to be involved in anything strange or \\nmysterious, because they just didn’t hold with such \\nnonsense.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMK1tkDtYzf7"
      },
      "source": [
        "def tokenize_words(input_text):\n",
        "    #rudimentary split into sentences\n",
        "    #input_text = input_text.split('.')\n",
        "    #input_text\n",
        "    #split into a list of words per sentence\n",
        "    tokens = [item.split() for item in input_text]\n",
        "    tokens\n",
        "    #lower case and put into a list of words independent of sentence\n",
        "    words = [word for sentence in tokens for word in sentence]\n",
        "    words = [word.lower() for word in words]\n",
        "    words\n",
        "    return words"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coP1zUEaYzf8"
      },
      "source": [
        "words = tokenize_words(input_text)\n",
        "#words"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUtSPvpGZyG_",
        "outputId": "41065cdf-f83c-42c4-abab-f4da23f42035"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3VKFhipYzf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b933b4c3-7c7f-4a33-9cf0-6822b67781c7"
      },
      "source": [
        "import nltk\n",
        "default_st = nltk.sent_tokenize\n",
        "_sentences = default_st(text=input_text)\n",
        "_sentences"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\"Mr. and Mrs. Dursley, of number four, Privet Drive, \\nwere proud to say that they were perfectly normal, \\nthank you very much.',\n",
              " 'They were the last people you’d \\nexpect to be involved in anything strange or \\nmysterious, because they just didn’t hold with such \\nnonsense.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TuWfio1Yzf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b344dfda-6cce-4e24-c2f8-c38e1fa35297"
      },
      "source": [
        "words = tokenize_words(_sentences)\n",
        "wordsr, cr = remove_stopwords(words)\n",
        "print(\"Here is the text:\", wordsr)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here is the text: ['\"mr.', 'mrs.', 'dursley,', 'number', 'four,', 'privet', 'drive,', 'proud', 'say', 'perfectly', 'normal,', 'thank', 'much.', 'last', 'people', 'you’d', 'expect', 'involved', 'anything', 'strange', 'mysterious,', 'didn’t', 'hold', 'nonsense.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5lRlqYsBYzf9",
        "outputId": "fb8330ad-aa55-485e-89a2-ea22623afd76"
      },
      "source": [
        "filtered_text = remove_stopwordsx(input_text)\n",
        "filtered_text"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" Mr. Mrs. Dursley , number four , Privet Drive , proud say perfectly normal , thank much. last people ’ expect involved anything strange mysterious , ’ hold nonsense .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQRtdT7qYzf_"
      },
      "source": [
        "\n",
        "3.\tRun one of the stemmers available in Python. Run the same two sentences from question 2 above through the stemmer and show the results. How many of the outputted stems are valid morphological roots of the corresponding words? Express this answer as a percentage.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5AJZO8tYzgA"
      },
      "source": [
        "# Porter Stemmer\n",
        "def porter_stemmer(input_list):\n",
        "    from nltk.stem import PorterStemmer\n",
        "    ps = PorterStemmer()\n",
        "    output_list = []\n",
        "    [output_list.append(ps.stem(x)) for x in input_list]\n",
        "    return output_list\n",
        "    #ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')\n",
        "\n",
        "# Lancaster Stemmer\n",
        "def lancaster_stemmer(input_list):\n",
        "    from nltk.stem import LancasterStemmer\n",
        "    ls = LancasterStemmer()\n",
        "    output_list = []\n",
        "    [output_list.append(ls.stem(x)) for x in input_list]\n",
        "    return output_list\n",
        "    #ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped')  \n",
        "\n",
        "# Regex based stemmer\n",
        "def regex_stemmer(input_list):\n",
        "    from nltk.stem import RegexpStemmer\n",
        "    rs = RegexpStemmer('ing$|s$|ed$', min=4)\n",
        "    output_list = []\n",
        "    [output_list.append(rs.stem(x)) for x in input_list]\n",
        "    return output_list\n",
        "    #rs.stem('jumping'), rs.stem('jumps'), rs.stem('jumped')   \n",
        "\n",
        "# Snowball Stemmer\n",
        "def snowball_stemmer(input_list):\n",
        "    from nltk.stem import SnowballStemmer\n",
        "    ss = SnowballStemmer(\"english\")\n",
        "    output_list = []\n",
        "    [output_list.append(ss.stem(x)) for x in input_list]\n",
        "    return output_list\n",
        "    #ss.stem('interstate') "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD5KNeSfr9Qj",
        "outputId": "3ae9d7f8-7757-4647-8805-3832d07a16c0"
      },
      "source": [
        "import string\n",
        "wordsr = [''.join(c for c in s if c not in string.punctuation) for s in wordsr]\n",
        "wordsr"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mr',\n",
              " 'mrs',\n",
              " 'dursley',\n",
              " 'number',\n",
              " 'four',\n",
              " 'privet',\n",
              " 'drive',\n",
              " 'proud',\n",
              " 'say',\n",
              " 'perfectly',\n",
              " 'normal',\n",
              " 'thank',\n",
              " 'much',\n",
              " 'last',\n",
              " 'people',\n",
              " 'you’d',\n",
              " 'expect',\n",
              " 'involved',\n",
              " 'anything',\n",
              " 'strange',\n",
              " 'mysterious',\n",
              " 'didn’t',\n",
              " 'hold',\n",
              " 'nonsense']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxs6ZNq2YzgB"
      },
      "source": [
        "pstem_list = porter_stemmer(wordsr)\n",
        "lancstem_list = lancaster_stemmer(wordsr)\n",
        "regstem_list = regex_stemmer(wordsr)\n",
        "snowstem_list = snowball_stemmer(wordsr)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpzHOm3Ifath",
        "outputId": "7f80fc14-0337-4d0e-b572-4970408ecd98"
      },
      "source": [
        "regstem_list"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mr',\n",
              " 'mrs',\n",
              " 'dursley',\n",
              " 'number',\n",
              " 'four',\n",
              " 'privet',\n",
              " 'drive',\n",
              " 'proud',\n",
              " 'say',\n",
              " 'perfectly',\n",
              " 'normal',\n",
              " 'thank',\n",
              " 'much',\n",
              " 'last',\n",
              " 'people',\n",
              " 'you’d',\n",
              " 'expect',\n",
              " 'involv',\n",
              " 'anyth',\n",
              " 'strange',\n",
              " 'mysteriou',\n",
              " 'didn’t',\n",
              " 'hold',\n",
              " 'nonsense']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksnxDppYf4qq",
        "outputId": "28c44604-808c-46d7-b1c1-ac87c5005440"
      },
      "source": [
        "denominator = len(regstem_list)\n",
        "denominator"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EF1v0GwkB7Q",
        "outputId": "c2d36592-847e-48d5-dd9f-3d423e8fb368"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "IkUskZxujxKE",
        "outputId": "e5751257-91ff-4f36-b2fd-9fe331c65c81"
      },
      "source": [
        "df = pd.DataFrame(regstem_list, columns= ['word'])\n",
        "for index, row in df.iterrows():\n",
        "    #print(row['word'])\n",
        "    test = wn.morphy(row['word'])\n",
        "    #print(test)\n",
        "    df.at[index, 'counter'] = 1\n",
        "    if test == None:\n",
        "      #print(\"not found\")\n",
        "      df.at[index, 'found'] = 0\n",
        "      df.at[index, 'morph'] = test\n",
        "    else:\n",
        "      df.at[index, 'found'] = 1\n",
        "      df.at[index, 'morph'] = test\n",
        "df      "
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>counter</th>\n",
              "      <th>found</th>\n",
              "      <th>morph</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mr</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>mr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mrs</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>mrs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dursley</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>number</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>number</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>four</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>four</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>privet</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>privet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>drive</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>drive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>proud</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>proud</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>say</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>say</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>perfectly</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>perfectly</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>normal</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>thank</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>thank</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>much</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>much</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>last</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>last</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>people</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>people</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>you’d</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>expect</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>expect</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>involv</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>anyth</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>strange</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>strange</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>mysteriou</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>didn’t</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>hold</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>hold</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>nonsense</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>nonsense</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         word  counter  found      morph\n",
              "0          mr      1.0    1.0         mr\n",
              "1         mrs      1.0    1.0        mrs\n",
              "2     dursley      1.0    0.0       None\n",
              "3      number      1.0    1.0     number\n",
              "4        four      1.0    1.0       four\n",
              "5      privet      1.0    1.0     privet\n",
              "6       drive      1.0    1.0      drive\n",
              "7       proud      1.0    1.0      proud\n",
              "8         say      1.0    1.0        say\n",
              "9   perfectly      1.0    1.0  perfectly\n",
              "10     normal      1.0    1.0     normal\n",
              "11      thank      1.0    1.0      thank\n",
              "12       much      1.0    1.0       much\n",
              "13       last      1.0    1.0       last\n",
              "14     people      1.0    1.0     people\n",
              "15      you’d      1.0    0.0       None\n",
              "16     expect      1.0    1.0     expect\n",
              "17     involv      1.0    0.0       None\n",
              "18      anyth      1.0    0.0       None\n",
              "19    strange      1.0    1.0    strange\n",
              "20  mysteriou      1.0    0.0       None\n",
              "21     didn’t      1.0    0.0       None\n",
              "22       hold      1.0    1.0       hold\n",
              "23   nonsense      1.0    1.0   nonsense"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xudMmenqq0uc",
        "outputId": "b3d3c1f5-6555-4ac1-92e2-136d4cebd0cc"
      },
      "source": [
        "percentage = sum(df['found']) / sum(df['counter']) * 100\n",
        "\n",
        "print(\"The percentage of words that are morphological roots is \"+ str(percentage) + \"%\")"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The percentage of words that are morphological roots is 75.0%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}